// Create data
val columns = Seq("language","users_count")
val data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000"))

// Spark Create DataFrame from RDD
val rdd = spark.sparkContext.parallelize(data)

// Create DataFrame from RDD
import spark.implicits._
val dfFromRDD1 = rdd.toDF()
dfFromRDD1.printSchema()
dfFromRDD1.show()

// Create DataFrame with custom column names
val dfFromRDD1 = rdd.toDF("language","users_count")
dfFromRDD1.show()
dfFromRDD1.printSchema()

// Using createDataFrame()
val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)

// Additional Imports
import org.apache.spark.sql.types.ArrayType
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.Row

// Create StructType Schema
val schema = StructType( Array(
                 StructField("language", StringType,true),
                 StructField("users", StringType,true)
             ))

// Use map() transformation to get Row type
val rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))
val dfFromRDD3 = spark.createDataFrame(rowRDD,schema)


// Import implicits
import spark.implicits._

// Create DF from data object
val dfFromData1 = data.toDF() 


// From Data (USING createDataFrame)
var dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)


// Import
import scala.collection.JavaConversions._

// From Data (USING createDataFrame and Adding schema using StructType)
val rowData= Seq(Row("Java", "20000"), 
               Row("Python", "100000"), 
               Row("Scala", "3000"))
var dfFromData3 = spark.createDataFrame(rowData,schema)


// Create Spark DataFrame from CSV
val df2 = spark.read.csv("/resources/file.csv")

// Creating from text (TXT) file
val df2 = spark.read
    .text("/resources/file.txt")
val df2 = spark.read
    .json("/resources/file.json")

spark-shell --packages com.databricks:spark-xml_2.12:0.18.0

val df = spark.read
  .format("com.databricks.spark.xml")
  .option("rowTag", "person")
  .load("resources/persons.xml")

df.show()










